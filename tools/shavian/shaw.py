#!/bin/python3

# Runs standard input through a part-of-speech tagger, then
# translates to Shavian. This resolves most heteronyms, but
# do still check the output for @ signs and fix them by hand.

# Each line of the dictionary consists of an English word, a space,
# and the Shavian transcription thereof. Comments are not allowed.
# Special notations are:
#
# ^word		word is a prefix
# $word		word is a suffix
# Word		word must always receive a naming dot
# word.		word must never receive a naming dot
# word_VB	word must be shaved this way when tagged as a verb
#
# A period in the Shavian word means that it takes no affixes.
# Use this for foreign words that don't take English affixes.
# Scoring problems should be addressed by adding longer morphemes.
#
# Words are matched case-sensitive when possible, thereby
# distinguishing "WHO" from "who" and "Nice" from "nice".
#
# shaw.py does not care about the order of dictionary entries.
# shaw.c requires all prefixes, then all suffixes, then all roots
# sorted case-insensitive, with underscores sorted before letters.

import sys
from html.parser import HTMLParser

import nltk

apostrophe = "'"  # whatever you want for apostrophe, e.g. "â€™" or ""
htags = {}
tokens = []
script = 0


def notrans(str):
    global htags
    tok = len(tokens)
    if tok in htags:
        htags[tok] += str
    else:
        htags[tok] = str


class MyHTMLParser(HTMLParser):
    def handle_starttag(self, tag, attrs):
        global script
        out = "<" + tag
        for at in attrs:
            if at[0] == "charset":
                at = ("charset", "UTF-8")
            if at[0] == "content":
                at = ("content", "text/html; charset=UTF-8")
            out += " " + at[0]
            if type(at[1]) == str:
                out += '="' + at[1] + '"'
        out += ">"
        if tag == "noscript" or tag == "script" or tag == "style":
            script = 1
        notrans(out)

    def handle_endtag(self, tag):
        global script
        notrans("</" + tag + ">")
        if tag == "noscript" or tag == "script" or tag == "style":
            script = 0

    def handle_data(self, data):
        global script, tokens
        if script:
            notrans(data)
        else:
            tokens += nltk.word_tokenize(data)


# break a string into alphabetic and non-alphabetic parts
# an apostrophe or period is "alphabetic" only if it's between two letters
def alpha_split(str):
    list = []
    prev = -1
    for i in range(len(str)):
        alpha = (
            str[i].isalpha()
            or "'.".find(str[i]) + 1
            and i > 0
            and i < len(str) - 1
            and str[i - 1].isalpha()
            and str[i + 1].isalpha()
        )
        if alpha == prev:
            list[-1] += str[i]
        else:
            list += str[i]
        prev = alpha
    return list


# Search all the ways a word might appear in the dictionary
def lookup(word, pos):
    ret = ""
    low = word.lower()
    pos = "_" + pos
    for look in [
        low + pos,
        low + pos[:3],
        word,
        word + ".",
        low,
        low + ".",
        low + "_NN",
        low + "_NNS",
        word[0].upper() + word[1:],
        word[0].upper() + low[1:],
        word.upper(),
    ]:
        if look in dict:
            ret = dict[look]
            if ret.find(".") + 1:
                ret = ret.replace(".", "") if (word == whole) else ""
            if not ret:
                continue
            if (
                (word[0].isupper() or look[0].isupper())
                and (look[-1] != "." or word != whole)
                and ret[-1] > "z"
            ):
                ret = "Â·" + ret
            break
    return ret


def suffix_split(inp, pos, adj):
    long = len(inp)
    root = lookup(inp, pos)
    if root:
        return ((long + adj) ** 2, root)
    low = inp.lower()
    best = (0, "")
    for split in range(max(long - 9, 2), long):
        suff = "$" + low[split:]
        if suff not in dict or low[split - 1 : split + 1] in ["ee", "ss"]:
            continue
        if low[split:] == "ess" and low[split - 2] == low[split - 1]:
            continue
        if low[split:] == "ry" and "aeiouf".find(low[split - 1]) + 1:
            continue
        if low[split:] == "r" and "aeiou".find(low[split - 1]) + 1:
            continue
        if (
            low[split:] == "n"
            and "eio".find(low[split - 1]) + 1
            and low[split - 2] != low[split - 1]
        ):
            continue
        suff = dict[suff]
        for pess in range(2):
            if pess:
                word = inp[:split]
            elif low[split - 1] == "i" and low[split] != "i" and low[split:] != "s":
                word = inp[: split - 1] + "y"
            elif "aeioy".find(low[split]) + 1 and not "aeio".find(low[split - 1]) + 1:
                if (
                    low[split - 1] == low[split - 2]
                    and not "sh".find(low[split - 1]) + 1
                ):
                    word = inp[: split - 1]
                elif (
                    "cghlsuvz".find(low[split - 1]) + 1
                    or low[split] == "e"
                    or "aeiousy".find(low[split - 2]) + 1
                ) and ("cg".find(low[split - 1]) < 0 or "aou".find(low[split]) < 0):
                    word = inp[:split] + "e"
                else:
                    continue
            elif low[split - 2 : split] == "dg":
                word = inp[:split] + "e"
            else:
                continue
            root = suffix_split(word, "UNK", split - len(word))
            score = (long - split + adj) ** 2 + root[0] if root[0] else 0
            if score <= best[0]:
                continue
            root = root[1]
            if root[-2:] == "ğ‘©ğ‘¤" and "ğ‘¦ğ‘©ğ‘¼".find(suff[0]) + 1 and word[-2:] == "le":
                root = root[:-2] + "ğ‘¤"
            if root[-3:] == "ğ‘Ÿğ‘©ğ‘¥" and suff not in ["ğ‘›", "ğ‘Ÿ", "ğ‘¦ğ‘™"]:
                root = root[:-3] + "ğ‘Ÿğ‘¥"
            mid = root[-1] + suff[0]
            if mid == "ğ‘¦ğ‘©":
                mid = "ğ‘¾"
            if mid == "ğ‘¦ğ‘¼":
                mid = "ğ‘½"
            if mid == "ğ‘¤ğ‘¤" and len(suff) < 3:
                mid = "ğ‘¤"
            best = (score, root[:-1] + mid + suff[1:])
    if len(best[1]) > 1:
        word = best[1][:-1]
        end = best[1][-1]
        edz = "ğ‘›ğ‘Ÿ".find(end) + 1
        if ["", "ğ‘‘ğ‘›", "ğ‘•ğ‘–ğ‘—ğ‘Ÿğ‘ ğ‘¡"][edz].find(word[-1]) + 1:
            word += "ğ‘©"
        if edz and word[-1] < "ğ‘˜":
            end = chr(ord(end) - 10)
        word += end
        if word[-4:] == "ğ‘’ğ‘©ğ‘¤ğ‘¦" and "ğ‘¦ğ‘©".find(word[-5]) + 1:
            word = word[:-4] + "ğ‘’ğ‘¤ğ‘¦"
        best = (best[0], word)
    return best


def prefix_split(word, pos, ms):
    best = suffix_split(word, pos, 0)
    split = min(len(word) - 2, 7)
    for split in range(split, ms, -1):
        pref = "^" + word[:split].lower()
        if pref not in dict:
            continue
        root = prefix_split(word[split:], pos, 1)
        score = split**2 + root[0] if root[0] else 0
        if score > best[0]:
            dot = "Â·" if word[0].isupper() else ""
            pref = dict[pref]
            if pref[-1] == root[1][0] and pref[-2] == "ğ‘¦" and "ğ‘¤ğ‘¥ğ‘®ğ‘¯".find(pref[-1]) + 1:
                pref = pref[:-1]
            best = (score, pref + dot + root[1])
    return best


# These are specific to the way NLTK breaks up formal and informal contractions,
# so they don't belong in dave.dict:
cont = {
    "'s": "'ğ‘Ÿ",
    "'d": "'ğ‘›",
    "'ll": "'ğ‘¤",
    "'m": "'ğ‘¥",
    "'re": "'ğ‘¼",
    "'ve": "'ğ‘",
    "n't": "ğ‘¯'ğ‘‘",
}
dict = {
    "ai": "ğ‘±",
    "ca": "ğ‘’ğ‘­.",
    "gim": "ğ‘œğ‘¦ğ‘¥",
    "lem": "ğ‘¤ğ‘§ğ‘¥",
    "na": "ğ‘¯ğ‘©",
    "ta": "ğ‘‘ğ‘©.",
    "wo": "ğ‘¢ğ‘´",
}

if len(sys.argv) < 2:
    print("Usage:", sys.argv[0], "file1.dict file2.dict ...")
    exit()

at = 1
for fname in sys.argv[1:]:
    with open(fname) as df:
        for line in df:
            word = line.split()
            if at and word[0] in dict:
                dict[word[0]] += "@" + word[1]
            else:
                dict[word[0]] = word[1]
    at = 0

text = sys.stdin.read()
text = text.replace("â€™", "'").replace("&#8217;", "'").replace("&rsquo;", "'")
text = text.replace("'s", " 's").replace("'S", " 's")
if text.lower().find("<html") == -1:
    text = text.replace("\n\n", " PARABREAK. ")

parser = MyHTMLParser()
parser.feed(text)

tags = nltk.pos_tag(tokens) + [(" ", " ")]
out = ""
prev = ("", ".")
tok = 0
initial = True
for token in tags:
    if tok in htags:
        out += htags[tok]
    tok += 1
    #  print (token)
    if token[1] == "." or token[1] == ":" or token[1] == "``" or token[0] == "â€œ":
        initial = True
    if token[0] == "PARABREAK" or prev[0] == "PARABREAK":
        out += "\n"
        prev = token
        continue
    low = token[0].lower()
    if low in cont:
        apos = cont[low]
        if low == "'s":
            if "ğ‘ğ‘‘ğ‘’ğ‘“ğ‘”".find(out[-1]) + 1:
                apos = "'ğ‘•"
            if "ğ‘•ğ‘–ğ‘—ğ‘Ÿğ‘ ğ‘¡".find(out[-1]) + 1:
                apos = "'ğ‘©ğ‘Ÿ"
        if prev[0] == "do" and low == "n't":
            out = out[:-1] + "ğ‘´"
        out += apos.replace("'", apostrophe)
        continue
    befto = {"have": "ğ‘¨ğ‘“", "has": "ğ‘¨ğ‘•", "used": "ğ‘•ğ‘‘", "unused": "ğ‘•ğ‘‘", "supposed": "ğ‘•ğ‘‘"}
    if prev[0] in befto and low == "to":  # If "to" changes the meaning of the preceding
        i = out.rfind(tran[-2:])  # word, it also changes the pronunciation.
        out = out[:i] + befto[prev[0]] + out[i + 2 :]
    if prev[0] == "lives" and low == "matter":
        out = out[:-4] + "ğ‘¤ğ‘²ğ‘ğ‘Ÿ"
    if token[0][0].isalnum():
        out += " "
    if (
        prev[0] == "can"
        and low == "not"
        or prev[0] == "got"
        and low == "ta"
        or prev[0] == "lem"
        and low == "me"
        or prev[0] == "gim"
        and low == "me"
        or prev[0] == "gon"
        and low == "na"
        or prev[0] == "wan"
        and low == "na"
    ):
        out = out[:-2]
        token = (low, token[1])
    for word in alpha_split(token[0]):
        if word.find(".") + 1:  # "e.g.", "U.S.A.", etc.
            out += word
            continue
        if initial and word[0].isalpha():
            initial = False
            if len(word) == 1 or word[1].islower():
                word = word[0].lower() + word[1:]
        if word == "&":
            out += " ğ‘¯"
            continue
        tran = ""
        i = "dlo".find(word[0].lower())
        if i >= 0 and len(word) > 1 and word[1] == "'" and word.lower() != "o'er":
            tran = "ğ‘›ğ‘¤ğ‘´"[i] + apostrophe
            word = word[2:]
        whole = word
        root = prefix_split(word, token[1], 0)
        tran += root[1] if root[1] else word
        if tran.find("Â·") + 1:
            tran = "Â·" + tran.replace("Â·", "")
        out += tran
    prev = (low, token[1])

out = out.replace("`` ", ' "').replace("``", '"')
out = out.replace(" ''", '" ').replace("''", '"')
for x in ["â€œ", "â€˜", "(", "[", "{", "$"]:
    out = out.replace(x + " ", " " + x)
print(out)
